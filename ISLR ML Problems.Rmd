---
title: "ISLR ML Problems"
author: "Robbie Mc Guinness"
date: "12/11/2021"
output: github_document
---


# Chapter 2: Statistical Learning

### Question 8
```{r}
library(ISLR)
summary(College)
pairs(College[,2:6])
boxplot(College$Outstate~College$Private)
Elite<-rep("No",nrow(College))
Elite[College$Top10perc>50]<-"Yes"
Elite<-as.factor(Elite)
College<-data.frame(College,Elite)
summary(College$Elite)
boxplot(College$Outstate~College$Elite)
par(mfrow=c(2,2))
hist(College$Room.Board,breaks=25)
hist(College$Books,breaks=15)
hist(College$Apps,breaks=50)
hist(College$PhD,breaks=12)

```
  

### Question 9
  The variables Name and Origin are qualitative and the rest are quantitative.
  
```{r}
for (i in 1:7)
{cat("The range of", names(Auto)[i],"is",range(Auto[,i]),"\n")}
```
  
```{r}
for (i in 1:7)
{cat("The mean and standard deviation of",names(Auto)[i],"are",mean(Auto[,i]),"and",sd(Auto[,i]),"\n")}
```


```{r}
Auto2<-Auto[-c(10:85),]
for (i in 1:7)
{cat("Having removed the 10th through 85th rows; the range of", names(Auto2)[i],"is",range(Auto2[,i]),"\n and the mean and standard deviation of",names(Auto2)[i],"are",mean(Auto2[,i]),"and",sd(Auto2[,i]),".\n")}
```

```{r}
pairs(Auto[,1:7])
```

There appears to be strong positive linear relationships between horsepower and weight. Similarly, there is a positive approximately linear relationship between horsepower and displacement and between weight and displacement.

The predictors displacement, weight and horsepower seem to be the ones with the most obvious relationship to mpg. Each exhibits a decreasing weakly non-linear relationship with mpg.

### Question 10
   
```{r}
library(ISLR2)
cat("The number of rows in the Boston dataset is",nrow(Boston),".")

cat("The number of columns in the Boston dataset is",ncol(Boston),".")
```
The rows represent different suburbs of Boston and the columns represent various housing and area statistics for each suburb. 

```{r}
pairs(Boston[,1:6])
pairs(Boston[,7:13])
```

```{r}
par(mfrow=c(3,4))
for (i in 2:13)
{plot(Boston$crim,Boston[,i],ylab=names(Boston)[i])}
```

None of the predictors seem to have a correlation with the per capita crime rate

```{r}
par(mfrow=c(3,1))
hist(Boston$crim,breaks=30)
hist(Boston$tax,breaks=30)
hist(Boston$ptratio,breaks=30)
```

From the histograms it is clear to see that there is a small number of outliers that have very large crime rates. Similarly, there are a number of suburbs where property tax is appreacibly above all other suburbs. The pupil teacher ratio is reasonably uniform apart from one large peak just above 20. 

```{r}
cat("The number of suburbs bordering the Charles river is",sum(Boston$chas),"\n")

cat("The median pupil teacher ratio is",median(Boston$ptratio))

cat("The minimum median value of owner occupied homes is",min(Boston$medv),"and it occurs in the",which.min(Boston$medv),"suburb.\n")

medians<-rep(0,ncol(Boston))
for (i in 1:ncol(Boston))
{if (median(Boston[,i])==0)
     {medians[i]=mean(Boston[,i])}
else
{medians[i]=median(Boston[,i])}
cat("The",names(Boston)[i], "predictor value at this location is",round(Boston[which.min(Boston$medv),i]/medians[i],3),"times the median value of that predictor. \n")}

room7<-rep(0,nrow(Boston))
room7[Boston$rm>7]=1
cat("The number of suburbs that average more than 7 rooms per house is",sum(room7)," representing",round(sum(room7)/length(room7),3),"of suburbs  \n")

room8<-rep(0,nrow(Boston))
room8[Boston$rm>8]=1
cat("The number of suburbs that average more than 8 rooms per house is",sum(room8)," representing",round(sum(room8)/length(room8),3),"of suburbs  \n")
```

# Chapter 3: Linear Regression

## Question 8


```{r}
Auto.fit1<-lm(mpg~horsepower,data=Auto)
summary(Auto.fit1)
predict(Auto.fit1,data.frame(horsepower=(98)),interval="confidence")
predict(Auto.fit1,data.frame(horsepower=(98)),interval="prediction")
```

There is a negative relationship between mpg and horsepower. For each unit increase in horsepower the mpg decreases by `coef(Auto.fit1)[2]`. The confidence and prediction intervals for a horsepower of 98 are as given above.

```{r}
plot(Auto$horsepower,Auto$mpg)
abline(Auto.fit1)
par(mfrow=c(2,2))
plot(Auto.fit1)
```

##Question 9

```{r}
cor(Auto[,1:8])
Auto.fit2<-lm(mpg~.-name,data=Auto)
summary(Auto.fit2)
par(mfrow=c(2,2))
plot(Auto.fit2)
Auto.fit3<-lm(mpg~year+origin+year:origin,data=Auto)
summary(Auto.fit3)
Auto.fit4<-lm(mpg~year*weight,data=Auto)
summary(Auto.fit4)
Auto.fit5<-lm(mpg~log(weight),data=Auto)
summary(Auto.fit5)
Auto.fit6<-lm(mpg~poly(weight,3),data=Auto)
summary(Auto.fit6)

```

There is a relationship between the predictors and the response evidenced by the p-value of the F-statistic. The predictors year, origin, weight and displacement are statistically significant. For each unit increase in year the miles per gallon increases by 0.75.

There appears to be a systematic trend to the residuals in the first plot indicating a mild non-linearity. Values at 323 and 326 are potentially outliers with standardised residuals close to 4 in absolute magnitude. Observation 14 is a reasonably high leverage point.

I've fitted two additional models - one with year and origin including an interaction and one with year and weight including an interaction. For the former only the year predictor is significant while for the latter both predictors and their interaction are significant.

There are also two non-linear regression models including weight; one with log(weight) as a predictor and the second with a polynomial up to third order. In the former the single predictor is significant while in the latter the linear and quadratic terms are significant.

## Question 10

```{r}
Carseats.fit1<-lm(Sales~Price+Urban+US,data=Carseats)
summary(Carseats.fit1)
Carseats.fit2<-lm(Sales~Price+US,data=Carseats)
summary(Carseats.fit2)
confint(Carseats.fit2)
par(mfrow=c(2,2))
plot(Carseats.fit2)
```
The price and whether the vendor is in the USA are statistically significant predictors. A one unit increase in carseat price will result in around 54 fewer
sales. Vendors based in the USA can expect to sell 1200 extra units.

Removing Urban makes little change to the coefficient estimates and the significance of the predictors. There does not appear to be any outliers or points with high leverage. 



# Chapter 4: Classification

## Question 13

```{r}

summary(Weekly)
pairs(Weekly[,-9])
weekly.fit<-glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Weekly,family=binomial)
summary(weekly.fit)
weekly.prob<-predict(weekly.fit,type="response")
weekly.pred<-rep("Down",dim(Weekly)[1])
weekly.pred[weekly.prob>0.5]="Up"
table(weekly.pred,Weekly$Direction)
weekly.train<-(Weekly$Year<2009)
Weekly.test<-Weekly[!weekly.train,]
Direction.test<-Weekly$Direction[!weekly.train]
weekly.fit2<-glm(Direction~Lag2,data=Weekly,family=binomial,subset=weekly.train)
weekly.prob2<-predict(weekly.fit2,Weekly.test,type="response")
weekly.pred2<-rep("Down",dim(Weekly.test)[1])
weekly.pred2[weekly.prob2>0.5]="Up"
table(weekly.pred2,Direction.test)
```

There appears to be a relationship between volume and year. In the logistic regression the only predictor that is significant is Lag2. The fraction of correct predictions on the test data using logistic regression is (9+56)/104=0.625.

```{r}
library(MASS)
weekly.fit3<-lda(Direction~Lag2,data=Weekly,subset=weekly.train)
weekly.prob3<-predict(weekly.fit3,Weekly.test)
weekly.class<-weekly.prob3$class
table(weekly.class,Direction.test)

```

Linear discriminant analysis also successfully predicts 0.625 of cases correctly.

```{r}
weekly.fit4<-qda(Direction~Lag2,data=Weekly,subset=weekly.train)
weekly.prob4<-predict(weekly.fit4,Weekly.test)
weekly.classqda<-weekly.prob4$class
table(weekly.classqda,Direction.test)
```

Quadratic discriminant analysis correctly predicts 61/104 instances representing 0.586 of cases.

```{r}
library(class)
train.knnx<-data.frame(Weekly$Lag2[weekly.train])
test.knnx<-data.frame(Weekly$Lag2[!weekly.train])
train.Direction<-Weekly$Direction[weekly.train]
set.seed(1)
knnweekly<-knn(train.knnx,test.knnx,train.Direction,k=1)
table(knnweekly,Direction.test)
```
K nearest neighbours with k=1 correctly predicts 52/104=0.5 of the test instances.


```{r}
library(e1071)
nb.fit<-naiveBayes(Direction~Lag2,data=Weekly,subset=weekly.train)
nb.class<-predict(nb.fit,Weekly.test)
table(nb.class,Direction.test)
```

The naive Bayes classifier correctly identifies 61/104=0.586 instances in the test set.


The linear regression and linear discriminant analysis classifier perform best on the test data set.

## Question 14

```{r}
mpg01<-rep(0,dim(Auto)[1])
mpg01[Auto$mpg>median(Auto$mpg)]=1
Auto01<-data.frame(Auto,mpg01)
par(mfrow=c(2,2))
for (i in 2:8)
{boxplot(Auto01[,i]~mpg01,ylab=names(Auto01)[i])}
Auto01.train<-(Auto01$year<80)
Auto01.test<-Auto01[!Auto01.train,]
mpg01.test<-Auto01$mpg01[!Auto01.train]
```
Weight, horsepower and displacement seem to be the most important predictors for mpg01.


```{r}
ldaauto.fit<-lda(mpg01~weight+horsepower+displacement,data=Auto01,subset=Auto01.train)
ldaauto.fit.pred<-predict(ldaauto.fit,Auto01.test)$class
table(ldaauto.fit.pred,mpg01.test)
```
A linear discriminant analysis classifier based on weight, horsepower and displacement correctly classified 76/85=0.894 of test data instances meaning the error rate was 0.106. 

```{r}
qdaauto.fit<-qda(mpg01~weight+horsepower+displacement,data=Auto01,subset=Auto01.train)
qdaauto.fit.pred<-predict(qdaauto.fit,Auto01.test)$class
table(qdaauto.fit.pred,mpg01.test)
```
A quadratic discriminant analysis classifier based on weight, horsepower and displacement correctly classified 74/85=0.870 of test data instances meaning the error rate was 0.130. 

```{r}
lrauto.fit<-glm(mpg01~weight+horsepower+displacement,data=Auto01,family="binomial",subset=Auto01.train)
lrauto.prob<-predict(lrauto.fit,Auto01.test,type="response")
lrauto.pred<-rep(0,length(lrauto.prob))
lrauto.pred[lrauto.prob>0.5]=1
table(lrauto.pred,mpg01.test)
```
Logistic regression correctly identifies 70/85=0.823 test instances meaning the error rate is 0.177.

```{r}
nbauto.fit<-naiveBayes(mpg01~weight+horsepower+displacement,data=Auto01,subset=Auto01.train)
nbauto.pred<-predict(nbauto.fit,Auto01.test)
table(nbauto.pred,mpg01.test)
```
Naive Bayes correctly identifies 78/85=0.918 instances of the test data set meaning the error rate is 0.812.

```{r}
knnauto.train<-cbind(Auto01$weight,Auto01$horsepower,Auto01$displacement)[Auto01.train,]
knnauto.test<-cbind(Auto01$weight,Auto01$horsepower,Auto01$displacement)[!Auto01.train,]
set.seed(1)
knnauto.fit1<-knn(knnauto.train,knnauto.test,mpg01[Auto01.train],k=1)
table(knnauto.fit1,mpg01.test)
knnauto.fit2<-knn(knnauto.train,knnauto.test,mpg01[Auto01.train],k=2)
table(knnauto.fit2,mpg01.test)
knnauto.fit3<-knn(knnauto.train,knnauto.test,mpg01[Auto01.train],k=3)
table(knnauto.fit3,mpg01.test)
knnauto.fit4<-knn(knnauto.train,knnauto.test,mpg01[Auto01.train],k=4)
table(knnauto.fit4,mpg01.test)
knnauto.fit5<-knn(knnauto.train,knnauto.test,mpg01[Auto01.train],k=5)
table(knnauto.fit5,mpg01.test)
knnauto.fit6<-knn(knnauto.train,knnauto.test,mpg01[Auto01.train],k=6)
table(knnauto.fit6,mpg01.test)
```
K nearest neighbours with k=1 correctly identifies 68/85=0.8 instances in the test data set meaningt the error rate is 0.2. K=1 performs best out of all possibilities between 1 and 6. 

# Chapter 5: Resampling Methods

# Chapter 6: Linear Model Selection and Regularisation

# Chapter 7: Moving Beyond Linearity

# Chapter 8: Tree Based Methods

# Chapter 9: Support Vector Machines

# Chapter 10: Deep Learning

# Chapter 11: Survival Analysis and Censored Data

# Chapter 12: Unsupervised Learning

# Chapter 13: Multiple Testing

